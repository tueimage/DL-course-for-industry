{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural networks for segmentation of cardiac MR images\n",
    "<span style=\"font-size:9pt;\">\n",
    "author: MWLafarge (m.w.lafarge@tue.nl); affiliation: Eindhoven University of Technology; created: Feb 2020\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "In the previous exercises, we saw how to tackle a classification task, by modeling the target likelihood function by a __deep convolutional neural network__.\n",
    "In this exercise we will address a segmentation problem for which we want to train a deep learning model to produce a binary mask that isolate a specific structure of interest (in this case, LV\n",
    "endocardial contour). \n",
    "\n",
    "### Part 1\n",
    "The first part of the exercise consists in implementing a baseline model to solve the task at hand.\n",
    "You will be shown how to implement a popular convoluational network architecture called [\"U-net\" (Ronneberger et al.)](https://arxiv.org/abs/1505.04597) with __tf.keras__.\n",
    "This architecture enables to produce dense predictions and is thus very well suited to solve segmentation tasks.\n",
    "\n",
    "The implementation of this model will introduce the use of __up-sampling layers__, __concatenation layers__ and how to customize a loss function within the __tf.keras__ framework. You will learn how to evaluate the predicted segmentation masks on a hold-out validation set.\n",
    "\n",
    "\n",
    "### Part 2\n",
    "The second part of the exercise leaves you free to expend this baseline model in order to improve its performances. By the end of the session, you are expected to process a hold-out test set of the dataset for which the ground-truth masks will be kept secret.\n",
    "\n",
    "The organizers will evaluate the participant predictions at the end of the session, this will give you a glance of a real-world situation in which your trained model is applied on new unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environement setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "#%matplotlib inline\n",
    "\n",
    "# system libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# computational libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# utility functions for this exercise\n",
    "from utils_ex4 import plot_image_list, plot_image_batch, plot_image_sequence, plot_featureMaps, Monitoring, submit_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "The dataset of this exercise consist of functional cardiac MR images.\n",
    "The images were collected from a cohort of 50 patients, and were acquired in different imaging centers (involving different MRI scanners).\n",
    "These functional images are __short time sequences__ which show the heart motion and its different phases during the cardiac cycle.\n",
    "\n",
    "The images were registered based on their DICOM format of origin.\n",
    "The dataset is restricted to the __long axis__ view and includes __2-Chamber, 3-Chamber and 4-Chamber__ views.\n",
    "The LV endocardial contour was manually annotated such that all images have a binary mask are associated with them (contour:1, otherwise:0).\n",
    "\n",
    "\n",
    "For the purpose of this exercise, the dataset was split in 3 sets:\n",
    "- A __training set__ that consists of 28 patients with a total of 2776 frames.\n",
    "- A __validation set__ that consists of 10 patients with a total of 1330 frames.\n",
    "- A __test set__ that consists of 12 patients with a total of 1193 frames.\n",
    "\n",
    "All images are gray-scale in an 8-bit format, with their intensity rescaled to the range \\[0, 255\\]. Images were resampled to the shape of \\[352px, 352px\\]. All images of the training set and validation set where perturbed with random spatial transformations.\n",
    "\n",
    "Each image/mask pair has a unique __id__, that is available in a metadata list given for each dataset split. Each __id__ can be used to fetch the images/mask from their paths, and to access information such as the corresponding __patient id__, __chamber view__ or __index within the functional sequence__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BASE_DIR = os.getcwd() # current working directory\n",
    "\n",
    "# source paths\n",
    "PATH_BASE_DIR = \"/data/industry_course/CMR\"\n",
    "\n",
    "DATA_PATH_META_TRAIN   = PATH_BASE_DIR + os.sep + \"training/metadata.npz\"\n",
    "DATA_PATH_IMAGES_TRAIN = PATH_BASE_DIR + os.sep + \"training/images/{id}.npy\"\n",
    "DATA_PATH_MASKS_TRAIN  = PATH_BASE_DIR + os.sep + \"training/contourMasks/{id}.npy\"\n",
    "\n",
    "DATA_PATH_META_VALID   = PATH_BASE_DIR + os.sep + \"validation/metadata.npz\"\n",
    "DATA_PATH_IMAGES_VALID = PATH_BASE_DIR + os.sep + \"validation/images/{id}.npy\"\n",
    "DATA_PATH_MASKS_VALID  = PATH_BASE_DIR + os.sep + \"validation/contourMasks/{id}.npy\"\n",
    "\n",
    "DATA_PATH_META_TEST   = PATH_BASE_DIR + os.sep + \"test/metadata.npz\"\n",
    "DATA_PATH_IMAGES_TEST = PATH_BASE_DIR + os.sep + \"test/images/{id}.npy\"\n",
    "DATA_PATH_MASKS_TEST  = PATH_BASE_DIR + os.sep + \"test/contourMasks/{id}.npy\"\n",
    "\n",
    "\n",
    "# import metadata\n",
    "train_metadata = np.load(DATA_PATH_META_TRAIN)\n",
    "valid_metadata = np.load(DATA_PATH_META_VALID)\n",
    "test_metadata  = np.load(DATA_PATH_META_TEST)\n",
    "\n",
    "\n",
    "# split metadata\n",
    "train_imageIds_list     = train_metadata[\"global_id\"]\n",
    "train_patientIds_list   = train_metadata[\"patient_id\"]\n",
    "train_chamberNames_list = train_metadata[\"chamber_name\"]\n",
    "train_sequenceIds_list  = train_metadata[\"sequence_idx\"]\n",
    "\n",
    "valid_imageIds_list     = valid_metadata[\"global_id\"]\n",
    "valid_patientIds_list   = valid_metadata[\"patient_id\"]\n",
    "valid_chamberNames_list = valid_metadata[\"chamber_name\"]\n",
    "valid_sequenceIds_list  = valid_metadata[\"sequence_idx\"]\n",
    "\n",
    "test_imageIds_list     = test_metadata[\"global_id\"]\n",
    "test_patientIds_list   = test_metadata[\"patient_id\"]\n",
    "test_sequenceIds_list  = test_metadata[\"sequence_idx\"]\n",
    "\n",
    "\n",
    "# check\n",
    "print(\"Training set: #patients = {}; #images = {}\".format(np.unique(train_patientIds_list).shape[0], train_imageIds_list.shape[0]))\n",
    "print(\"Validation set: #patients = {}; #images = {}\".format(np.unique(valid_patientIds_list).shape[0], valid_imageIds_list.shape[0]))\n",
    "print(\"Test set: #patients = {}; #images = {}\".format(np.unique(test_patientIds_list).shape[0], test_imageIds_list.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's visualize a few examples.  As it can be seen, a binary mask is associated with each training and validation image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random list of image indices to fetch\n",
    "rdm_indices = [train_imageIds_list[np.random.randint(train_imageIds_list.shape[0])] for _i in range(8)]\n",
    "\n",
    "# scan the list of indices and import image-mask pairs\n",
    "batch_images = []\n",
    "batch_masks  = []\n",
    "for idx_c in rdm_indices:\n",
    "    img_path  = DATA_PATH_IMAGES_TRAIN.format(id=idx_c)\n",
    "    mask_path = DATA_PATH_MASKS_TRAIN.format(id=idx_c)\n",
    "    batch_images.append(np.load(img_path))\n",
    "    batch_masks.append(np.load(mask_path))\n",
    "\n",
    "# finally we visualize the imported images\n",
    "plot_image_batch(np.stack(batch_images), np.stack(batch_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: TRAINING A BASELINE U-NET MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap the task at hand: we want to create a model that takes actual tensor images of shape \\[352,352,1\\] (images with a single gray-level channel) as input and outputs the pixel-wise likelihood of the presence of the *LV\n",
    "endocardial contour* (i.e. a probability map)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the graph operations\n",
    "For this problem we want to make a pixel-to-pixel prediction. Having a patch-based approach would be one solution (input: image patch, output: prediction of its center pixel), but it is not efficient as it will require to exhaustively scan all possible patches to make a dense prediction (a predicted mask with a pixel-to-pixel mapping).\n",
    "\n",
    "[Ronneberger et al.](https://arxiv.org/abs/1505.04597) proposed a solution to this problem by the means of a specific network architecture: using skip-connections and up-sampling layers enables to output prediction with a pixel-to-pixel mapping for a large region of the input image. For more details, [see original paper](https://arxiv.org/abs/1505.04597).\n",
    "\n",
    "Implementing the architecture of a U-net can be done in 3 steps:\n",
    "- 1) constructing a __down-sampling branch__ of convolutional layers and max-pooling layers\n",
    "- 2) constructing an __up-sampling branch__ of convolutional layers and un-pooling layers towards a sigmoid-activated output\n",
    "- 3) concatenating the intermediate features maps of the __down-sampling branch__ to the inputs of the __up-sampling branch__.\n",
    "\n",
    "The required operations are:\n",
    "- an __tf.keras.Input()__ placeholder that takes image tensors of size \\[352, 352, 1\\] as input.\n",
    "- several [__tf.keras.layers.Conv2D()__](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layers to filter intermediate feature maps (convolution operation + non-linearity).\n",
    "- several [__tf.keras.layers.MaxPool2D()__](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers to down-sample the intermediate feature maps in the __down-sampling branch__.\n",
    "- several [__tf.keras.layers.UpSampling2D()__](https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D) layers to up-sample the intermediate feature maps in the __up-sampling branch__.\n",
    "- [__tf.keras.layers.Cropping2D()__](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Cropping2D) layers so that the shape of the concatenated intermediate feature maps match the shape of the inputs in the __up-sampling branch__.\n",
    "- [__tf.keras.layers.Concatenate()__](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate) layers to connect the __down-sampling branch__ to the __up-sampling branch__.\n",
    "\n",
    "As for the previous exercise, we will use a regularizer to prevent overfitting.\n",
    "It is *very* important to keep track of the change of shape of the features maps along the sequence of layers. This way, we make sure we know how to crop and concatenate the feature maps, and we make sure we know the expected shape of the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# placeholder for the image data\n",
    "image_size = [352, 352, 1]\n",
    "regularizer = None #tf.keras.regularizers.l2(l=0.0005)\n",
    "\n",
    "# model hyper-parameters\n",
    "N = 8 # number of feature maps in each convolutional layer\n",
    "M = 16 # number of feature maps in the last layer (prior to the output)\n",
    "\n",
    "# utilitary layers\n",
    "activation  = \"relu\"\n",
    "\n",
    "# input placeholder\n",
    "inputs = tf.keras.Input(shape=image_size)  #-- Placeholder for gray-scale input images\n",
    "\n",
    "\n",
    "# down-sampling paths\n",
    "conv_down1 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 5,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "maxPoolLayer1 = tf.keras.layers.MaxPool2D(\n",
    "    pool_size = (2, 2),\n",
    "    strides   = None,\n",
    "    padding   = \"valid\")\n",
    "\n",
    "conv_down2 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 3,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "maxPoolLayer2 = tf.keras.layers.MaxPool2D(\n",
    "    pool_size = (2, 2),\n",
    "    strides   = None,\n",
    "    padding   = \"valid\")\n",
    "\n",
    "conv_down3 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 3,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "maxPoolLayer3 = tf.keras.layers.MaxPool2D(\n",
    "    pool_size = (2, 2),\n",
    "    strides   = None,\n",
    "    padding   = \"valid\")\n",
    "\n",
    "conv_down4 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 3,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "maxPoolLayer4 = tf.keras.layers.MaxPool2D(\n",
    "    pool_size = (2, 2),\n",
    "    strides   = None,\n",
    "    padding   = \"valid\")\n",
    "\n",
    "conv_down5 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 3,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "upPoolLayer5  = tf.keras.layers.UpSampling2D(\n",
    "    size          = (2, 2),\n",
    "    interpolation = \"nearest\")\n",
    "\n",
    "# up-sampling path\n",
    "conv_up4 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 3,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "upPoolLayer4  = tf.keras.layers.UpSampling2D(\n",
    "    size          = (2, 2),\n",
    "    interpolation = \"nearest\")\n",
    "\n",
    "conv_up3 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 3,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "upPoolLayer3  = tf.keras.layers.UpSampling2D(\n",
    "    size          = (2, 2),\n",
    "    interpolation = \"nearest\")\n",
    "\n",
    "conv_up2 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 3,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "upPoolLayer2  = tf.keras.layers.UpSampling2D(\n",
    "    size          = (2, 2),\n",
    "    interpolation = \"nearest\")\n",
    "\n",
    "conv_up1 = tf.keras.layers.Conv2D(\n",
    "    filters     = M,\n",
    "    kernel_size = 3,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "\n",
    "# output layer\n",
    "layerOut = tf.keras.layers.Conv2D(\n",
    "    filters     = 1,\n",
    "    kernel_size = 1,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = \"sigmoid\",\n",
    "    kernel_regularizer = regularizer)\n",
    "\n",
    "\n",
    "# utilitary layers\n",
    "cropLayer1 = tf.keras.layers.Cropping2D(\n",
    "    cropping    = (44, 44),\n",
    "    input_shape = (348, 348, N))\n",
    "concatLayer1  = tf.keras.layers.Concatenate()\n",
    "\n",
    "cropLayer2 = tf.keras.layers.Cropping2D(\n",
    "    cropping    = (20, 20),\n",
    "    input_shape = (172, 172, N))\n",
    "concatLayer2  = tf.keras.layers.Concatenate()\n",
    "\n",
    "cropLayer3 = tf.keras.layers.Cropping2D(\n",
    "    cropping    = (8, 8),\n",
    "    input_shape = (84, 84, N))\n",
    "concatLayer3  = tf.keras.layers.Concatenate()\n",
    "\n",
    "cropLayer4 = tf.keras.layers.Cropping2D(\n",
    "    cropping    = (2, 2),\n",
    "    input_shape = (40, 40, N))\n",
    "concatLayer4  = tf.keras.layers.Concatenate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the graph and instantiating the model\n",
    "We can now join the graph components to define the model __output__ as a function of the __input placeholder__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph construction\n",
    "# down path\n",
    "\n",
    "x = conv_down1(inputs)\n",
    "x1_skip = cropLayer1(x)\n",
    "x = maxPoolLayer1(x)\n",
    "\n",
    "x = conv_down2(x)\n",
    "x2_skip = cropLayer2(x)\n",
    "x = maxPoolLayer2(x)\n",
    "\n",
    "x = conv_down3(x)\n",
    "x3_skip = cropLayer3(x)\n",
    "x = maxPoolLayer3(x)\n",
    "\n",
    "x = conv_down4(x)\n",
    "x4_skip = cropLayer4(x)\n",
    "x = maxPoolLayer4(x)\n",
    "\n",
    "# bottleneck\n",
    "x = conv_down5(x)\n",
    "x = upPoolLayer5(x)\n",
    "\n",
    "# up path\n",
    "x = concatLayer4([x4_skip, x])\n",
    "x = conv_up4(x)\n",
    "x = upPoolLayer4(x)\n",
    "\n",
    "x = concatLayer3([x3_skip, x])\n",
    "x = conv_up3(x)\n",
    "x = upPoolLayer3(x)\n",
    "\n",
    "x = concatLayer2([x2_skip, x])\n",
    "x = conv_up2(x)\n",
    "x = upPoolLayer2(x)\n",
    "\n",
    "x = concatLayer1([x1_skip, x])\n",
    "x = conv_up1(x)\n",
    "\n",
    "# output\n",
    "outputs = layerOut(x)\n",
    "\n",
    "# model definition\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom weighted cross-entropy\n",
    "\n",
    "In this dataset, the pixel-wise instances are imbalanced (we have far less foreground pixels than background pixels). If we use a typical cross-entropy loss, all pixel instances will have the same weight, giving more importance to the classification of the background pixel than for the foreground pixels.\n",
    "A solution consists in __customizing__ the cross-entropy loss (to make a weighted binary cross-entropy loss) such that we calculate and apply a coefficient to artificially re-balance the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home-made loss\n",
    "def wBCE(y_true, y_pred):\n",
    "    y_true = tf.keras.backend.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "    nb_positive = tf.keras.backend.sum(y_true)\n",
    "    nb_negative = tf.keras.backend.sum(1.0 - y_true)\n",
    "    \n",
    "    freq_positive = (nb_positive + nb_negative) / nb_positive\n",
    "    freq_negative = (nb_positive + nb_negative) / nb_negative\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    cross_entropies_positive = -1.0 * y_true * tf.keras.backend.log(y_pred + epsilon)\n",
    "    cross_entropies_negative = -1.0 * (1.0 - y_true) * tf.keras.backend.log(1.0 - y_pred + epsilon)\n",
    "    \n",
    "    cross_entropies_w = freq_positive * cross_entropies_positive + freq_negative * cross_entropies_negative\n",
    "    cross_entropies_w = 0.5 * cross_entropies_w\n",
    "    \n",
    "    return tf.keras.backend.mean(cross_entropies_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to train the model\n",
    "\n",
    "Let's define an optimizer, here [Adam with default parameters works fine (see documentation)](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent with momentum\n",
    "# optimizer = tf.keras.optimizers.SGD(\n",
    "#     learning_rate = 0.01,\n",
    "#     momentum      = 0.9)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate = 0.001,\n",
    "    beta_1        = 0.9,\n",
    "    beta_2        = 0.999,\n",
    "    epsilon       = 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "We finally configure the model for training by indicating the loss, the optimizer, and check the model architecture we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss      = wBCE,\n",
    "    metrics   = None)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a data generator\n",
    "\n",
    "Again, for memory-wise and training efficiency reasons, we will define generators that can be called to produce __mini-batches__ of samples when needed.\n",
    "\n",
    "Images are sampled randomly withing their split, they keep their tensor shape, and we will rescale their intensity to be in \\[0,1\\].   \n",
    "Note that the masks need to be cropped in order to match the shape of the network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_images_ids,\n",
    "                 batchSize=1,\n",
    "                 path_images_formated = DATA_PATH_IMAGES_TRAIN,\n",
    "                 path_masks_formated  = DATA_PATH_MASKS_TRAIN):\n",
    "        \n",
    "        self.batch_size = batchSize\n",
    "        self._path_images_formated = path_images_formated\n",
    "        self._path_masks_formated  = path_masks_formated\n",
    "        \n",
    "        self.data_images_ids = data_images_ids\n",
    "        self.data_size       = data_images_ids.shape[0]\n",
    "        self.data_indices_shuffle = np.arange(self.data_size)\n",
    "        np.random.shuffle(self.data_indices_shuffle)\n",
    "        \n",
    "        self.scan_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.data_size / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_images  = []\n",
    "        batch_masks   = []\n",
    "        \n",
    "        for _i in range(self.batch_size):\n",
    "            idx_local = self.data_indices_shuffle[self.scan_idx]\n",
    "            img_id_c = self.data_images_ids[idx_local]\n",
    "            \n",
    "            img_path  = self._path_images_formated.format(id=img_id_c)\n",
    "            mask_path = self._path_masks_formated.format(id=img_id_c)\n",
    "            \n",
    "            img_c  = np.load(img_path)\n",
    "            mask_c = np.load(mask_path)\n",
    "\n",
    "            batch_images.append(img_c)\n",
    "            batch_masks.append(mask_c)\n",
    "        \n",
    "            self.scan_idx += 1\n",
    "            self.scan_idx %= self.data_size\n",
    "            \n",
    "        batch_images = np.stack(batch_images, axis=0)\n",
    "        batch_images = np.expand_dims(batch_images, axis=3)\n",
    "        \n",
    "        slice_crop  = slice(47, -47, 1) #! The target mask need to be cropped to match the shape of the output of the network\n",
    "        batch_masks = np.stack(batch_masks, axis=0)\n",
    "        batch_masks = batch_masks[:, slice_crop, slice_crop]\n",
    "        batch_masks = np.expand_dims(batch_masks, axis=3)\n",
    "        \n",
    "        batch_weights = batch_masks\n",
    "        \n",
    "        batch_images = batch_images / 255.0 #-- [0,1] scaling\n",
    "        return batch_images, batch_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can instantiate 2 generators: one to generate mini-batches of the training data, one to generate mini-batches of the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGenerator_train = dataGenerator(train_imageIds_list, batchSize=2)\n",
    "dataGenerator_valid = dataGenerator(valid_imageIds_list[:10], batchSize=10,\n",
    "        path_images_formated = DATA_PATH_IMAGES_VALID,\n",
    "        path_masks_formated  = DATA_PATH_MASKS_VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets check what keras receives when the generator are called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitrary_iteration_idx = 42\n",
    "train_images_batch, train_masks_batch = dataGenerator_train[arbitrary_iteration_idx]\n",
    "\n",
    "print(\"Mini-batch of images has shape: \", train_images_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and monitoring the training process\n",
    "\n",
    "Now that our generators are instantiated, we can train our model. We will use __tf.keras.Model.fit\\_generator__ function to start the training procedure by feeding the data generators.   \n",
    "To get a better insight and to keep track of the status of the learning process, the Monitoring callback for this exercise will show the predicted mask of a pre-selected validation image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import a random validation image and instanciate the monitoring callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a validation image/mask\n",
    "img_id_c  = valid_imageIds_list[0]\n",
    "img_path  = DATA_PATH_IMAGES_VALID.format(id=img_id_c)\n",
    "mask_path = DATA_PATH_MASKS_VALID.format(id=img_id_c)\n",
    "\n",
    "img_valid_c  = np.load(img_path)\n",
    "mask_valid_c = np.load(mask_path)\n",
    "\n",
    "monitor = Monitoring(\n",
    "    model = model,\n",
    "    layerTracking = None,\n",
    "    validImage = img_valid_c,\n",
    "    validMask  = mask_valid_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the task of this exercise is more complex and training is more time consuming. It can be good to use a __Checkpoint callback__, that can store checkpoints of the network during training in memory, and that can be re-loaded later on if necessary. For more details on the checkpoint callback of __tf.keras__, [see the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare callback to store checkpoints of the model\n",
    "checkpoint_path = os.getcwd() + os.sep + \"checkpoint_baseline.hdf5\"\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_path,\n",
    "    monitor  = 'val_loss',\n",
    "    verbose  = 0,\n",
    "    \n",
    "    save_best_only = True,\n",
    "    mode = \"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally run the training procedure (including all the generators and callbacks we defined before.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbEpochs  = 500\n",
    "model.fit_generator(\n",
    "    generator       = dataGenerator_train,\n",
    "    steps_per_epoch = 10,\n",
    "    epochs          = nbEpochs,\n",
    "\n",
    "    validation_data  = dataGenerator_valid,\n",
    "    validation_freq  = 5,\n",
    "\n",
    "    verbose   = 1,\n",
    "    callbacks = [monitor, checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitatitve evaluation: visualizing some predictions\n",
    "Before evaluating the model, we can check qualitatively that the trained model is doing a good job on hold-out images. Let's select a sample from the validation set and compare the predictions with the ground-truth contour masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first we use the test generator to create batches of test images\n",
    "arbitrary_iteration_idx = 42\n",
    "batch_valid_images, batch_valid_masks = dataGenerator_valid[arbitrary_iteration_idx]\n",
    "batch_valid_images = batch_valid_images[:8] # Selection of a small sample\n",
    "batch_valid_masks  = batch_valid_masks[:8]\n",
    "\n",
    "# then we get the prediction of the mode\n",
    "tensor_predictions = model.predict(\n",
    "    x = batch_valid_images)\n",
    "\n",
    "# we format the results for visualization\n",
    "batch_valid_images = batch_valid_images[:,:,:,0]\n",
    "tensor_predictions = tensor_predictions[:,:,:,0]\n",
    "\n",
    "print(\"Validation Images and Predicted Masks\")\n",
    "plot_image_batch(batch_valid_images, tensor_predictions) #-- We finally visualize the results\n",
    "\n",
    "#print(\"Validation Images and Ground-Truth Masks\")\n",
    "#plot_image_batch(batch_valid_images, batch_valid_masks) #-- We finally visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the intermediate feature maps\n",
    "To get an intuition of the learned features, intermediate feature maps can be visualized by defining an auxiliary model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define an auxillary model\n",
    "\n",
    "model_aux = tf.keras.Model(inputs, x1_skip) # first conv layer as a target\n",
    "#model_aux = tf.keras.Model(inputs, x2_skip) # second conv layer as a target\n",
    "#model_aux = tf.keras.Model(inputs, x3_skip) # third conv layer as a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first select a random training image\n",
    "img_id_c  = train_imageIds_list[np.random.randint(len(train_imageIds_list))]\n",
    "img_path  = DATA_PATH_IMAGES_TRAIN.format(id=img_id_c)\n",
    "\n",
    "demo_image = np.load(img_path)\n",
    "demo_image = demo_image.astype(np.float) / 255.0 # [0,1] rescaling\n",
    "\n",
    "demo_image = np.expand_dims(demo_image, axis=0) # enforce the shape of a batch of 1 image\n",
    "demo_image = np.expand_dims(demo_image, axis=3) # enforce the shape of a batch of 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we feed the selected image to the auxillary network\n",
    "featureMaps = model_aux.predict(demo_image) # returns a tensor with shape [1,Height,Width,Channels]\n",
    "\n",
    "plot_featureMaps(demo_image, featureMaps, maxNbChannels = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative evaluation on the validation Set\n",
    "Now that we have a trained model, we would like to evaluate its performance on the validation set that was not used to train the model.\n",
    "A metric commonly used to measure the difference between two binary maps is the [DICE coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) (also called $F_1$-score). The metric we will use in this exercise is the mean DICE coefficient across all the instances of the validation set.\n",
    "\n",
    "\n",
    "Note that the raw generated predicted mask are a pixel-wise class label likelihood and are continuous values in \\[0,1\\]. Therefore we need to binarise the prediction in order to be compared (and the DICE coefficient is defined for 2 binary maps). A straight-forward solution is to threshold the network output with a fixed threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_threshold = 0.9 # manually defined binarization threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DICE(maskA, maskB):\n",
    "    # make sure masks are binary maps\n",
    "    maskA = maskA.astype(np.bool)\n",
    "    maskB = maskB.astype(np.bool)\n",
    "    \n",
    "    # DICE coefficient definition: A ^ B / (|A| + |B|)\n",
    "    interAB = maskA * maskB\n",
    "    score   = 2 * np.sum(interAB) / (np.sum(maskA) + np.sum(maskB))\n",
    "    return score\n",
    "\n",
    "def pad_mask(source, target):\n",
    "    target_h, target_w   = target.shape\n",
    "    source_h, source_w   = source.shape\n",
    "    pad_y = (target_h - source_h) // 2\n",
    "    pad_x = (target_w - source_w) // 2\n",
    "    \n",
    "    return np.pad(source, ([pad_y,pad_y], [pad_x,pad_x]))\n",
    "\n",
    "def import_valid_data(valid_id):\n",
    "    # Import the image\n",
    "    img_path  = DATA_PATH_IMAGES_VALID.format(id=valid_id)\n",
    "    img_c = np.load(img_path) / 255.0\n",
    "\n",
    "    # Import the ground-truth mask\n",
    "    mask_path  = DATA_PATH_MASKS_VALID.format(id=valid_id)\n",
    "    mask_c = np.load(mask_path)  \n",
    "    \n",
    "    return img_c, mask_c\n",
    "\n",
    "# select the id of a validation data point\n",
    "random_index = np.random.randint(valid_imageIds_list.shape[0])\n",
    "img_id = valid_imageIds_list[random_index] \n",
    "\n",
    "img_c, mask_true = import_valid_data(img_id)\n",
    "\n",
    "# extend to fit the input placeholder of the network\n",
    "img_c_ext = np.expand_dims(img_c, axis=0)\n",
    "img_c_ext = np.expand_dims(img_c_ext, axis=3)\n",
    "\n",
    "# compute the prediction\n",
    "mask_pred = model.predict(\n",
    "     x = img_c_ext)\n",
    "mask_pred = mask_pred[0,:,:,0] # Remove undesired dimensions\n",
    "\n",
    "# pad the predicted mask to match the input dimensions\n",
    "mask_pred = pad_mask(mask_pred, mask_true)\n",
    "\n",
    "# binarize the predicted mask\n",
    "mask_pred_bin = mask_pred > prediction_threshold\n",
    "\n",
    "plot_image_list(\n",
    "    [mask_true, mask_pred_bin, mask_true * mask_pred_bin],\n",
    "    [\"True Contour\", \"Predicted Contour\", \"Intersection\"])\n",
    "\n",
    "# compute DICE SCORE\n",
    "dice = DICE(mask_true, mask_pred_bin)\n",
    "print(\"Dice score:\", dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the full validation set\n",
    "Let's compute the mean DICE coefficient on the validation set by scanning all its data points *(this can take a few minutes)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DICE(maskA, maskB):\n",
    "    # make sure masks are binary maps\n",
    "    maskA = maskA.astype(np.bool)\n",
    "    maskB = maskB.astype(np.bool)\n",
    "    \n",
    "    # DICE coefficient definition: A ^ B / (|A| + |B|)\n",
    "    interAB = maskA * maskB\n",
    "    score   = 2 * np.sum(interAB) / (np.sum(maskA) + np.sum(maskB))\n",
    "    return score\n",
    "\n",
    "def pad_mask(source, target):\n",
    "    target_h, target_w   = target.shape\n",
    "    source_h, source_w   = source.shape\n",
    "    pad_y = (target_h - source_h) // 2\n",
    "    pad_x = (target_w - source_w) // 2\n",
    "    \n",
    "    return np.pad(source, ([pad_y,pad_y], [pad_x,pad_x]))\n",
    "\n",
    "def import_valid_data(valid_id):\n",
    "    # import the image\n",
    "    img_path  = DATA_PATH_IMAGES_VALID.format(id=valid_id)\n",
    "    img_c = np.load(img_path) / 255.0\n",
    "\n",
    "    # import the ground-truth mask\n",
    "    mask_path  = DATA_PATH_MASKS_VALID.format(id=valid_id)\n",
    "    mask_c = np.load(mask_path)  \n",
    "    \n",
    "    return img_c, mask_c\n",
    "\n",
    "score_list = []\n",
    "for idx_scan, img_id in enumerate(valid_imageIds_list):\n",
    "    img_c, mask_true = import_valid_data(img_id)\n",
    "\n",
    "    # extend to fit the input placeholder of the network\n",
    "    img_c_ext = np.expand_dims(img_c, axis=0)\n",
    "    img_c_ext = np.expand_dims(img_c_ext, axis=3)\n",
    "\n",
    "\n",
    "    # compute the prediction\n",
    "    mask_pred = model.predict(\n",
    "         x = img_c_ext)\n",
    "    mask_pred = mask_pred[0,:,:,0] # Remove undesired dimensions\n",
    "\n",
    "    # pad the predicted mask to match the input dimensions\n",
    "    mask_pred = pad_mask(mask_pred, mask_true)\n",
    "\n",
    "    # binarize the predicted mask\n",
    "    mask_pred_bin = mask_pred > prediction_threshold\n",
    "\n",
    "    # compute DICE SCORE\n",
    "    dice = DICE(mask_true, mask_pred_bin)\n",
    "    \n",
    "    score_list.append(dice)\n",
    "    \n",
    "    print(\"[Scan validation set {}/{}] Running average DICE {}\".format(idx_scan+1, valid_imageIds_list.shape[0], np.mean(score_list)), end=\"\\r\")\n",
    "\n",
    "print(\"\\nAverage DICE:\", np.mean(score_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: IMPROVING THE MODEL AND MINI-CHALLENGE SUBMISSION\n",
    "Now that you have completed PART 1 of the exercise, you should have a baseline that produces reasonable performance, but by no means perfect.\n",
    "\n",
    "In this PART 2, your goal is to expend and improve this baseline model. Below is a list of suggestions for improvements.\n",
    "\n",
    "## SUGGESTIONS FOR IMPROVEMENT\n",
    "- Investigating other pre-processing methods (by changing the generator so as to apply a normalization/whitening transformation on the network inputs).\n",
    "- Investigating other data-sampling approaches (by changing the generator so that inputs follow another sampling-distribution. For instance a patient-based distribution might be more realistic and more likely to generalize).\n",
    "- Investigating different model architecture (by changing the network hyperparameters, adding layers \n",
    "etc. See (batchNorm layers)[https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization].)\n",
    "- Investigating other optimizers and parametrizations (see (learning rate schedulling)[https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler])\n",
    "- Investigating other regularizers (see (tf.keras.regularizers)[https://www.tensorflow.org/api_docs/python/tf/keras/regularizers])\n",
    "- Investigating post-processing approaches (ideally, the model output should be optimal but sometimes transforms can be found to refine the predictions).\n",
    "- Searching optimal hyperparamters (for instance, you can write a script to find the optimum binarization threshold on the test set and use it on the test set).\n",
    "- ... \n",
    "\n",
    "## MINI-CHALLENGE DESCRIPTION\n",
    "Towards the end of the session you can run your best model on the __test set__ and make a submission for the mini-challenge via the __submit_results()__ function.\n",
    "At the end of the session, the organizers will evaluate the participants submissions on the hold-out ground-truth contours mask and will release the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative evaluation on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a qualitative idea of the predictions made on the test set, you can visualize the predictions made for a whole CMR dynamic sequence using the function __plot_image_sequence()__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let choose the id of a specific patient\n",
    "print(\"List of test patient ids: \", np.unique(test_metadata[\"patient_id\"]))\n",
    "selected_patient_id = 42\n",
    "patient_indices = np.where(test_metadata[\"patient_id\"] == selected_patient_id)[0]\n",
    "\n",
    "# Select the 30 first images for this patient (they are already ordered)\n",
    "image_indices   = test_imageIds_list[patient_indices][:30] \n",
    "\n",
    "# Import the images and create a tensor to be fed to the network\n",
    "batch_images_test = []\n",
    "for idx_c in image_indices:\n",
    "    img_path  = DATA_PATH_IMAGES_TEST.format(id=idx_c)\n",
    "    batch_images_test.append(np.load(img_path))\n",
    "\n",
    "# Format the tensor\n",
    "batch_images_test = np.stack(batch_images_test)\n",
    "batch_images_test = batch_images_test / 255.0 # [0,1] rescaling\n",
    "batch_images_test = np.expand_dims(batch_images_test, axis=3)\n",
    "\n",
    "# Feed the tensor to the model\n",
    "tensor_predictions = model.predict(\n",
    "    x = batch_images_test)\n",
    "\n",
    "# Finally we generate an animation of the image sequence, next to the predicted segmentation mask\n",
    "stopAnimationFunction = plot_image_sequence(batch_images_test[:,:,:,0], tensor_predictions[:,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"font-size:15pt;color:red\"> *To prevent the notebook from crashing due to the animation, you can stop the animation by executing the next cell.* </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopAnimationFunction() # Stop the animation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to evaluate the full test set and submit the results\n",
    "Let's scan the whole test set and compute predictions (this can take a few minutes), and then submit the compiled results (and wait for a confirmation message)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_mask(source, target):\n",
    "    target_h, target_w   = target.shape[:2]\n",
    "    source_h, source_w   = source.shape[:2]\n",
    "    pad_y = (target_h - source_h) // 2\n",
    "    pad_x = (target_w - source_w) // 2\n",
    "    \n",
    "    return np.pad(source, ([pad_y,pad_y], [pad_x,pad_x]))\n",
    "\n",
    "def import_test_data(test_id):\n",
    "    # Import the image\n",
    "    img_path  = DATA_PATH_IMAGES_TEST.format(id=test_id)\n",
    "    img_c = np.load(img_path) / 255.0\n",
    "    \n",
    "    return img_c\n",
    "\n",
    "test_result_dict = {}\n",
    "for idx_scan, img_id in enumerate(test_imageIds_list[:]):\n",
    "    img_c = import_test_data(img_id)\n",
    "\n",
    "    # Extend to fit the input placeholder of the network\n",
    "    img_c_ext = np.expand_dims(img_c, axis=0)\n",
    "    img_c_ext = np.expand_dims(img_c_ext, axis=3)\n",
    "\n",
    "    # Compute the prediction\n",
    "    mask_pred = model.predict(\n",
    "         x = img_c_ext)\n",
    "    mask_pred = mask_pred[0,:,:,0] # Remove undesired dimensions\n",
    "\n",
    "    # Pad the predicted mask to match the input dimensions\n",
    "    mask_pred = pad_mask(mask_pred, img_c)\n",
    "    \n",
    "    # Binarize the predicted mask\n",
    "    prediction_threshold = 0.8\n",
    "    mask_pred_bin = mask_pred > prediction_threshold\n",
    "    \n",
    "    test_result_dict[img_id] = mask_pred_bin\n",
    "    \n",
    "    print(\"[Scanning test set {}/{}]\".format(idx_scan+1, test_imageIds_list.shape[0]), end=\"\\r\")\n",
    "\n",
    "print(\"\\n Test results, ready are to submit:\", len(test_result_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally the dictionary of results can be submitted (wait for confirmation)\n",
    "received_results = submit_results(test_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to check that binary mask were correctly submitted.\n",
    "ex_mask = received_results[\"4220\"]\n",
    "plot_image_list(\n",
    "    [ex_mask],\n",
    "    [\"Checking Test Prediction\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
