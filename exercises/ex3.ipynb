{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep convolutional neural networks for image classification\n",
    "<span style=\"font-size:9pt;\">\n",
    "author: MWLafarge (m.w.lafarge@tue.nl); affiliation: Eindhoven University of Technology; created: Feb 2020\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "In the previous exercise, we saw how to tackle a classification task by modeling the target likelihood function by a fully-connected neural network. In order to exploit the structured nature of image data, we will implement a deep convolutional neural network (CNN) by replacing dense layers by __convolution layers__. This should result in a more more efficient model with significantly smaller number of parameters. \n",
    "\n",
    "This exercise will also show how to efficiently reduce the dimensionality of the feature maps along the depth of the networks by the use of __pooling layers__. You will see how to visualize the learned convolutional kernels, intermediate feature maps and get insights of the reasons why CNNs are a powerful machine framework for image processing and analysis.\n",
    "\n",
    "This exercise follows the same structure as the second exercise and uses the same classification problem as an example. The main differences are in the \"Defining the graph operations\" section and the \"Visualizing the intermediate feature maps\" subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environement setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "# system libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# computational libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# utility functions for this exercise\n",
    "from utils_ex3 import plot_image_batch, plot_featureMaps, Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "\n",
    "The dataset for this exercise is derived from the __PCam__ benchmark dataset created by B. Veeling et al. ([github.com/basveeling/pcam](https://github.com/basveeling/pcam)) that in turn originates from the [CAMELYON16 challenge](https://camelyon16.grand-challenge.org/). \n",
    "\n",
    "The PCam dataset consist of RGB color images of size 96 $\\times$ 96 pixels extracted from histological sections of sentinel lymph node' tissue of breast cancer patients. Each image is annotated with a binary label indicating presence of metastatic tissue in the patch.\n",
    "\n",
    "For the purpose of this exercise, the original PCam dataset was subsampled to 20000 training images and 2000 test images, balanced across the two classes. Furthermore, to enable faster processing the images were cropped to the central region of 64 $\\times$ 64 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BASE_DIR = os.getcwd() # current working directory\n",
    "\n",
    "def load_data_helper(path):\n",
    "    data_dict = np.load(path) \n",
    "    data_points = data_dict[\"images\"]\n",
    "    data_labels = data_dict[\"labels\"]\n",
    "    \n",
    "    return data_points, data_labels\n",
    "\n",
    "path_train = PATH_BASE_DIR + os.sep + \"../data/data_smallPCam_training.npz\"\n",
    "path_test  = PATH_BASE_DIR + os.sep + \"../data/data_smallPCam_test.npz\"\n",
    "data_images_train, data_labels_train = load_data_helper(path_train)\n",
    "data_images_test, data_labels_test   = load_data_helper(path_test)\n",
    "    \n",
    "print(\"Imported training points: \", data_images_train.shape)\n",
    "print(\"Imported training labels: \", data_labels_train.shape)\n",
    "\n",
    "print(\"Imported test points: \", data_images_test.shape)\n",
    "print(\"Imported test labels: \", data_labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's visualize a few examples.  As it can be seen, the class label of a patch (1: tumor, 0: not tumor) is not obvious to non-experts (this task requires experience with analyzing histopathology images). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = [np.random.randint(data_images_train.shape[0]) for _i in range(8)]\n",
    "\n",
    "batch_images = data_images_train[random_indices,]\n",
    "batch_labels = data_labels_train[random_indices,]\n",
    "\n",
    "plot_image_batch(batch_images, batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a classifier\n",
    "We want to create a model that takes actual tensor images of shape \\[64,64,3\\] (64 $\\times$ 64 RGB image) as input and outputs the likelihood of the class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the graph operations\n",
    "As with the previous exercises, let's first instantiate all the components that we will use to construct the network. This time, instead of fully connected layers, we will use convolutional layers, that require specific parameters. The network will be a straight-forward sequence of alternating convolutional layers and max-pooling layers until a bottleneck is reached. Finally, a fully connected layer will produce the network output.\n",
    "\n",
    "- an __tf.keras.Input()__ placeholder that takes image tensors of size \\[64, 64, 3\\] as input.\n",
    "- several [__tf.keras.layers.Conv2D()__](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layers to filter intermediate feature maps (convolution operation + non-linearity).\n",
    "- several [__tf.keras.layers.MaxPool2D()__](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers to down-sample the intermediate feature maps.\n",
    "- an __tf.keras.layers.Dense()__ layer that will constitute the final layer of the network and activated by a sigmoid.\n",
    "\n",
    "As for the previous exercise, we will use a regularizer to prevent overfitting.\n",
    "It is important to keep track of the change of shape of the features maps along the sequence of layers: the shape is indicated, in comment, at the instanciation of each layer.\n",
    "This way, a bottleneck is reached after the fifth convolutional layer (spatial dimensions are reduced to \\[1,1\\]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# regularizer object\n",
    "weight_decay = 0.0001\n",
    "regularizer = tf.keras.regularizers.l2(l=weight_decay)\n",
    "\n",
    "# placeholder for the image data\n",
    "image_size = [64, 64, 3]\n",
    "inputs = tf.keras.Input(shape=image_size)\n",
    "\n",
    "# model hyper-parameters\n",
    "N = 8 # number of feature maps in each convolutional layer\n",
    "M = 16 # number of feature maps in the last layes (prior to the output)\n",
    "activation = \"relu\" # chosen non-linerarity for the convolutional layers\n",
    "\n",
    "# model components\n",
    "# feature map shape: [64,64,3] -> [60,60,N]\n",
    "conv1 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 5,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "\n",
    "# feature map shape: [60,60,16] -> [30,30,16]\n",
    "maxPool1 = tf.keras.layers.MaxPool2D(\n",
    "    pool_size = (2, 2),\n",
    "    strides   = None,\n",
    "    padding   = \"valid\")\n",
    "\n",
    "# feature map shape: [30,30,N] -> [28,28,N]\n",
    "conv2 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 3,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "\n",
    "# feature map shape: [28,28,N] -> [14,14,N]\n",
    "maxPool2 = tf.keras.layers.MaxPool2D(\n",
    "    pool_size = (2, 2),\n",
    "    strides   = None,\n",
    "    padding   = \"valid\")\n",
    "\n",
    "# [14,14,N] -> [12,12,N]\n",
    "conv3 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 3,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "\n",
    "# [12,12,N] -> [6,6,N]\n",
    "maxPool3 = tf.keras.layers.MaxPool2D(\n",
    "    pool_size = (2, 2),\n",
    "    strides   = None,\n",
    "    padding   = \"valid\")\n",
    "\n",
    "# [6,6,N] ->  [4,4,N]\n",
    "conv4 = tf.keras.layers.Conv2D(\n",
    "    filters     = N,\n",
    "    kernel_size = 3,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "\n",
    "# [4,4,N] -> [2,2,N]\n",
    "maxPool4 = tf.keras.layers.MaxPool2D(\n",
    "    pool_size = (2, 2),\n",
    "    strides   = None,\n",
    "    padding   = \"valid\")\n",
    "\n",
    "# [2,2,N] -> [1,1,M]\n",
    "conv5 = tf.keras.layers.Conv2D(\n",
    "    filters     = M,\n",
    "    kernel_size = 2,\n",
    "    strides     = (1, 1),\n",
    "    padding     = \"valid\",\n",
    "    activation  = activation,\n",
    "    kernel_regularizer = regularizer)\n",
    "\n",
    "# [1,1,M] -> [M]\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "# [M] ->  [1]\n",
    "layerOut = tf.keras.layers.Dense(\n",
    "    units       = 1,\n",
    "    activation  = \"sigmoid\",\n",
    "    kernel_regularizer = regularizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the graph and instantiating the model\n",
    "We can now join the graph components to define the model __output__ as a function of the __input placeholder__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureMap1 = maxPool1(conv1(inputs))\n",
    "featureMap2 = maxPool2(conv2(featureMap1))\n",
    "featureMap3 = maxPool3(conv3(featureMap2))\n",
    "featureMap4 = maxPool4(conv4(featureMap3))\n",
    "featureMap5 = flatten(conv5(featureMap4))\n",
    "outputs = layerOut(featureMap5)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to train the model\n",
    "\n",
    "As before, we will use binary cross entropy loss and stochastic gradient descent with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy loss between the distribution of ground truth labels and the model predictions\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# stochastic gradient descent with momentum\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate = 0.01,\n",
    "    momentum      = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "We finally configure the model for training by indicating the loss, the optimizer and performance metrics to be computed during training.  \n",
    "\n",
    "We use __model.summary()__ to display and check the model architecture we implemented.\n",
    "Remark that with a similar architecture, the total number of trainable parameters in the network is highly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss      = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics   = [\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a data generator\n",
    "\n",
    "Again, for memory-wise and training efficiency reasons, we will define generators that can be called to produce __mini-batches__ of samples when needed.\n",
    "\n",
    "This time, images will keep their tensor shape, and we will rescale their intensity to be in \\[0,1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\" DataGenerator herited from tf.keras.utils.Sequence\n",
    "        Input: image data, label data\n",
    "        __getitem__: Returns random samples (mini-batches) drawn from the data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_images, data_labels):\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.data_images = data_images\n",
    "        self.data_labels = data_labels\n",
    "        self.data_size   = data_images.shape[0]\n",
    "        \n",
    "        self.scan_idx    = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.data_size / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for _i in range(self.batch_size):\n",
    "            batch_images.append(self.data_images[self.scan_idx,])\n",
    "            batch_labels.append(self.data_labels[self.scan_idx])\n",
    "        \n",
    "            self.scan_idx += 1 # Loop over available images\n",
    "            self.scan_idx %= self.data_size\n",
    "            \n",
    "        batch_images = np.stack(batch_images, axis=0)\n",
    "        batch_labels = np.array(batch_labels)\n",
    "        \n",
    "        batch_images = batch_images / 255.0 # Images are rescaled in [0,1]\n",
    "        \n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can instantiate 2 generators: one to generate mini-batches of the training data, one to generate mini-batches of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGenerator_train = dataGenerator(data_images_train, data_labels_train)\n",
    "dataGenerator_test  = dataGenerator(data_images_test, data_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets check what keras receives when the generator are called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitrary_iteration_idx = 42\n",
    "train_images_batch, train_labels_batch = dataGenerator_train[arbitrary_iteration_idx]\n",
    "\n",
    "print(\"Mini-batch of images has shape: \", train_images_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and monitoring the training process\n",
    "\n",
    "Now that our generators are instantiated, we can train our model. We will use __tf.keras.Model.fit\\_generator__ function to start the training procedure by feeding data generators instead of data arrays. Look at the documentation of [tf.keras.Model.fit\\_generator](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\\_generator) for more details.\n",
    "\n",
    "To get a better insight of the learned kernels, the Monitor callback for this exercise will show visualizations of the kernels of the first layer. Note that the output of each neuron in the first layer is: kernel $\\ast$ image + bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nbEpochs  = 500\n",
    "\n",
    "model.fit_generator(\n",
    "    generator = dataGenerator_train,\n",
    "    steps_per_epoch = 10,\n",
    "    epochs          = nbEpochs,\n",
    "\n",
    "    validation_data  = dataGenerator_test,\n",
    "    validation_freq  = 5,\n",
    "    validation_steps = 1,\n",
    "\n",
    "    verbose   = 1,\n",
    "    callbacks = [Monitoring(layerTracking=conv1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "Once the model is trained, we want to quantify its performances on the hold-out test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We evaluate the model on test data\n",
    "eval_out = model.evaluate_generator(\n",
    "    generator = dataGenerator_test,\n",
    "    verbose   = 1)\n",
    "\n",
    "print(\"Accuracy on test set: \", eval_out[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing some predictions\n",
    "Let's select a sample of test images and compare the predictions with the true class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we use the test generator to create batches of test images\n",
    "arbitrary_iteration_idx = 42\n",
    "batch_test_images, batch_test_labels = dataGenerator_test[arbitrary_iteration_idx]\n",
    "batch_test_images = batch_test_images[:8] # Selection of a small sample\n",
    "batch_test_labels = batch_test_labels[:8]\n",
    "\n",
    "# Then we get the prediction of the mode\n",
    "tensor_predictions = model.predict(\n",
    "    x = batch_test_images)\n",
    "\n",
    "# We format the results in a list for visualization\n",
    "list_results = [\"True Class [{}] \\n P(y=1|x) = {}\".format(true_y, str(pred_y[0])[:5]) \n",
    "                for true_y, pred_y in zip(batch_test_labels, tensor_predictions)]\n",
    "\n",
    "# We rescale the images to their range of origin\n",
    "batch_test_images = 255.0 * batch_test_images #-- Rescaling\n",
    "batch_test_images = batch_test_images.astype(np.uint8) #-- 8-bit conversion\n",
    "\n",
    "plot_image_batch(batch_test_images, list_results) #-- We finally visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the intermediate feature maps\n",
    "The nature of the convolution operation enable to conserve the spatial structure of the input images withing the intermediate feature maps. It is possible to extract and visualize them for a given input. Although they can be difficult to interpret, they reveal how the input images are transformed to produce the obtained output.\n",
    "\n",
    "This can be implemented with __tf.keras__ by creating a new auxilary __tf.keras.Model__ that ouputs the intermediate map of interest.\n",
    "We can see the returned feature maps with the function __plot_featureMaps__ (from ex3_utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a auxillary model\n",
    "\n",
    "model_aux = tf.keras.Model(inputs, featureMap1) # first conv layer as a target\n",
    "#model_aux = tf.keras.Model(inputs, featureMap2) # second conv layer as a target\n",
    "#model_aux = tf.keras.Model(inputs, featureMap3) # third conv layer as a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first select a random training image\n",
    "random_index = np.random.randint(data_images_train.shape[0])\n",
    "demo_image = data_images_train[random_index,]\n",
    "demo_image = np.expand_dims(demo_image, axis=0) # enforce the shape of a batch of 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We feed the selected image to the auxillary network\n",
    "featureMaps = model_aux.predict(demo_image) # returns a tensor with shape [1,Height,Width,Channels]\n",
    "\n",
    "plot_featureMaps(demo_image, featureMaps, maxNbChannels = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## The effect of the model architecture\n",
    "\n",
    "Experiment with different neural network architectures and observe the training process (using the training and test loss curves), the kernels of the first layer (using the visualization of the kernels) and the classification accuracy\n",
    "You can create new neural network architectures by varying the width of the network (__number of channels__), changing the __pooling operations__ (average pooling, global pooling).  \n",
    "\n",
    "You can try varying the __kernel size__, __padding__ or __stride__ parameters of the convolutionl layers. Observe the effect on the learning curves and intermediate feature maps. Make sure that the shape of the feature maps stays coherent and that the full network always produces a single scalar output for a given input image.\n",
    "\n",
    "## The effect of the training procedure and regularization\n",
    "\n",
    "Experiment with different parameters of the optimizer(learning rate and the momentum) and observe the training process using the training and test loss curves. Try varying the $L_2$ regularization factor. What is the effect of increasing the regularization on the appearance of the convolutional kernels? You can also experiment with using different optimizers (a list of supported optimizers is available [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
