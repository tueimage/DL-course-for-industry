{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression and simple neural networks\n",
    "<span style=\"font-size:9pt;\">\n",
    "author: MWLafarge (m.w.lafarge@tue.nl); affiliation: Eindhoven University of Technology; created: Feb 2020\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "In this exercise you will implement a logistic regression model using the __Keras__ framework with a Tensorflow backend.  \n",
    "The documentation of the Tensorflow implementation of the Keras API that will be used in the exercises can be found [here](https://www.tensorflow.org/api_docs/python/tf/keras).\n",
    "\n",
    "The main goal of this first exercise is to get familiar with the functional Keras API and develop an intuition of training a logistic regression model for a toy dataset. Furthermore, this notebook demonstrates how to implement the components of a machine-learning model with __tf.keras__, how to train and test the model, and how to interpret the learning process by visualizing the training and testing loss curves. Then, in the exercises you will use this knowledge to solve two more difficult classification problems by extending the logistic regression classifier to a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environement setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "# system libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# computational libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# utility functions for this exercise\n",
    "from utils_ex1 import plot_2d_dataset, plot_learned_landscape, Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset\n",
    "\n",
    "Load the datasets that are stored as numpy arrays. The datasets consist of pairs of 2D __features__ (points in 2D space) and binary class __labels__. \n",
    "\n",
    "The arrays containing the features have the following dimensions: \\[number of samples, number of features\\].  \n",
    "\n",
    "The arrays containing the labels have the following dimensions: \\[number of samples\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BASE_DIR = os.getcwd() # current working directory\n",
    "\n",
    "def load_data_helper(path):\n",
    "    data_dict = np.load(path) \n",
    "    data_points = data_dict[\"points\"]\n",
    "    data_labels = data_dict[\"labels\"]\n",
    "    \n",
    "    return data_points, data_labels\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    path_train = PATH_BASE_DIR + os.sep + \"../data/data_toy_{}_training.npz\".format(dataset_name)\n",
    "    path_test  = PATH_BASE_DIR + os.sep + \"../data/data_toy_{}_test.npz\".format(dataset_name)\n",
    "    data_points_train, data_labels_train = load_data_helper(path_train)\n",
    "    data_points_test, data_labels_test = load_data_helper(path_test)\n",
    "    \n",
    "    return data_points_train, data_labels_train, data_points_test, data_labels_test\n",
    "\n",
    "data_points_train, data_labels_train, data_points_test, data_labels_test = load_data(\"blobs\")\n",
    "\n",
    "print(\"Imported training points: \", data_points_train.shape)\n",
    "print(\"Imported training labels: \", data_labels_train.shape)\n",
    "\n",
    "print(\"Imported test points: \", data_points_test.shape)\n",
    "print(\"Imported test labels: \", data_labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be visualized in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_2d_dataset(data_points_train, data_labels_train, data_points_test, data_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a classifier\n",
    "\n",
    "We want to train a logistic regression model that takes 2D features as input and outputs the likelihood of the class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the graph operations\n",
    "In symbolic programming, we want to model the desired likelihood function as a sequence of operations (that forms a graph/network) from an input placeholder object to produce an output.\n",
    "\n",
    "We need first to instantiate all the components we might want to use to construct the network:\n",
    "- __tf.keras.Input()__ to define the input placeholder.  \n",
    "- __tf.keras.layers.Dense()__ to define densely connected layers (2D matrix multiplication + non-linear activation).\n",
    "\n",
    "Since we are training a logistic regression model, only and input and output layer need to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer: samples with two features are expected\n",
    "inputs = tf.keras.Input(shape=(2)) \n",
    "\n",
    "# output layer with sigmoid activation\n",
    "layerOut = tf.keras.layers.Dense(1, activation=\"sigmoid\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the graph and instantiating the model\n",
    "We can now join the graph components to define the model __output__ as a function of the __input placeholder__. Then we can instantiate the model with __tf.keras.Model__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "outputs = layerOut(inputs) \n",
    "\n",
    "# instanciate the full model with the input-output objects\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing to train the model\n",
    "\n",
    "In order to train the model, we need to define a __loss function__ that we want to optimize, and the gradient-descent procedure (__optimizer__) we want to use to update the weights of the model during training.  \n",
    "\n",
    "The __tf.keras__ library comes with a module with pre-defined standard loss functions (__tf.keras.losses__) and a module with standard optimization algorithms (__tf.keras.optimizers__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-entropy loss between the distribution of ground truth labels and the model predictions\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# stochastic gradient descent with momentum\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate = 0.01,\n",
    "    momentum      = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "We finally configure the model for training by indicating the loss, the optimizer and performance metrics to be computed during training.  \n",
    "\n",
    "We use __model.summary()__ to display and check the model architecture we implemented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss      = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics   = [\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and monitoring the training process\n",
    "Now that the model is compiled, it is ready to be trained. The __tf.keras.Model.fit__ function enables starting the training process in a single call.\n",
    "The function arguments we use in this exercises are:\n",
    "- __x, y__: numpy arrays representing the training dataset features (x) and targets (y)\n",
    "- __epochs__: the number of training epoch (the number of time the full training set is processed)\n",
    "- __batch_size__: the size of the mini-batches\n",
    "- __validation_data__: tuple of validation (test) features and labels (__x,y__)\n",
    "- __validation_freq__: frequency with which to evaluate the model on the test subset\n",
    "- __callbacks__: a list of objects inherited from __tf.keras.Callback__ that can be called automatically during the training process. Here the __Monitoring()__ callback is used to monitor the training and test losses during training. This is a custom function that we have implemented for these exercises.\n",
    "\n",
    "More details can be found in the documentation of [__tf.keras.Model.fit__](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbEpochs  = 300\n",
    "batchSize = 32\n",
    "\n",
    "# fit the model to our data\n",
    "model.fit(\n",
    "    x = data_points_train,\n",
    "    y = data_labels_train,\n",
    "    \n",
    "    epochs     = nbEpochs,\n",
    "    batch_size = batchSize,\n",
    "\n",
    "    validation_data = (data_points_test[:32,], data_labels_test[:32,]),\n",
    "    validation_freq = 10,\n",
    "\n",
    "    verbose   = 0,\n",
    "    callbacks = [Monitoring(x_range=[0, 10000], refresh_steps=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "Once the model is trained, we want to quantify its performances on the hold-out test set. \n",
    "The __tf.keras.Model.evaluate__ function returns a list including the value of the loss function and the values of the chosen metrics (defined previously when calling __tf.keras.Model.compile__) for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on test data\n",
    "eval_out = model.evaluate(\n",
    "    x = data_points_test,\n",
    "    y = data_labels_test,\n",
    "    verbose = 0)\n",
    "\n",
    "print(\"Accuracy on test set: \", eval_out[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better idea of the learned function we can visualize its output on the 2D plane using the function __plot_learned_landscape__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_learned_landscape(model, data_points_train, data_labels_train, data_points_test, data_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Solving the circle (or bullseye) dataset\n",
    "\n",
    "The circle dataset can be loaded and visualized in the following way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points_train, data_labels_train, data_points_test, data_labels_test = load_data(\"circle\")\n",
    "plot_2d_dataset(data_points_train, data_labels_train, data_points_test, data_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a neural network with one hidden layer that can solve this classification problem.  \n",
    "You can use the following template to write and test your solution, which is very similar to the code above used to train logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "# TODO: define the layers of your neural network\n",
    "# TODO: connect all the layers from an input object to an output object\n",
    "\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "# instanciate the full model with the input-output objects\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# cross-entropy loss between the distribution of ground truth labels and the model predictions\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# stochastic gradient descent with momentum\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate = 0.01,\n",
    "    momentum      = 0.9)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss      = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics   = [\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "nbEpochs  = 300\n",
    "batchSize = 32\n",
    "\n",
    "# we fit the model to our data\n",
    "model.fit(\n",
    "    x = data_points_train,\n",
    "    y = data_labels_train,\n",
    "    \n",
    "    epochs     = nbEpochs,\n",
    "    batch_size = batchSize,\n",
    "\n",
    "    validation_data = (data_points_test[:32,], data_labels_test[:32,]),\n",
    "    validation_freq = 10,\n",
    "\n",
    "    verbose   = 0,\n",
    "    callbacks = [Monitoring(x_range=[0, 10000], refresh_steps=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the learned function\n",
    "plot_learned_landscape(model, data_points_train, data_labels_train, data_points_test, data_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the spiral dataset\n",
    "\n",
    "Now, attempt to solve the spiral dataset that can be loaded and visualized in the following way: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points_train, data_labels_train, data_points_test, data_labels_test = load_data(\"spiral\")\n",
    "plot_2d_dataset(data_points_train, data_labels_train, data_points_test, data_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find the smallest neural network that can give an accurate solution. Note that you might also have to modify the learning rate of the stochastic gradient descent algorithm and/or increase the total training time (number of epochs) in order to arrive fast at an acceptable solution. You can use the following code template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "# TODO: define the layers of your neural network\n",
    "# TODO: connect all the layers from an input object to an output object\n",
    "\n",
    "#-------------------------------------------------------------------------#\n",
    "\n",
    "# instanciate the full model with the input-output objects\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# cross-entropy loss between the distribution of ground truth labels and the model predictions\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# stochastic gradient descent with momentum\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate = 0.01,\n",
    "    momentum      = 0.9)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss      = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics   = [\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "nbEpochs  = 300\n",
    "batchSize = 32\n",
    "\n",
    "# we fit the model to our data\n",
    "model.fit(\n",
    "    x = data_points_train,\n",
    "    y = data_labels_train,\n",
    "    \n",
    "    epochs     = nbEpochs,\n",
    "    batch_size = batchSize,\n",
    "\n",
    "    validation_data = (data_points_test[:32,], data_labels_test[:32,]),\n",
    "    validation_freq = 10,\n",
    "\n",
    "    verbose   = 0,\n",
    "    callbacks = [Monitoring(x_range=[0, 10000], refresh_steps=10)])\n",
    "\n",
    "plot_learned_landscape(model, data_points_train, data_labels_train, data_points_test, data_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "As it is now, the spiral dataset is relatively difficult to overfit since the classes are well separated and there is a large number of training samples.  \n",
    "We can simulate an even more difficult problem by adding some noise to the data and subsampling the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points_train, data_labels_train, data_points_test, data_labels_test = load_data(\"spiral\")\n",
    "\n",
    "data_points_train = data_points_train[::2,:]\n",
    "data_labels_train = data_labels_train[::2]\n",
    "\n",
    "data_points_train = data_points_train + np.random.uniform(high=0.8, size=data_points_train.shape)\n",
    "data_points_test  = data_points_test  + np.random.uniform(high=0.8, size=data_points_test.shape)\n",
    "\n",
    "plot_2d_dataset(data_points_train, data_labels_train, data_points_test, data_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will train a relatively large neural network with this dataset.  \n",
    "Due to the noisy nature of the data and the smaller number of samples the network will start to overfit.  \n",
    "Question: How can you diagnose that the model is overfiting from the training and testing loss curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(2)) \n",
    "\n",
    "layer1 = tf.keras.layers.Dense(100, activation=\"relu\")\n",
    "layer2 = tf.keras.layers.Dense(100, activation=\"relu\")\n",
    "layer3 = tf.keras.layers.Dense(100, activation=\"relu\")\n",
    "\n",
    "# output layer with sigmoid activation\n",
    "layerOut = tf.keras.layers.Dense(1, activation=\"sigmoid\") \n",
    "\n",
    "# neural network function\n",
    "outputs = layerOut(layer3(layer2(layer1(inputs))))\n",
    "\n",
    "# instanciate the full model with the input-output objects\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# cross-entropy loss between the distribution of ground truth labels and the model predictions\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# stochastic gradient descent with momentum\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate = 0.05,\n",
    "    momentum      = 0.9)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss      = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics   = [\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbEpochs  = 1000\n",
    "batchSize = 32\n",
    "\n",
    "# we fit the model to our data\n",
    "model.fit(\n",
    "    x = data_points_train,\n",
    "    y = data_labels_train,\n",
    "    \n",
    "    epochs     = nbEpochs,\n",
    "    batch_size = batchSize,\n",
    "\n",
    "    validation_data = (data_points_test[:32,], data_labels_test[:32,]),\n",
    "    validation_freq = 10,\n",
    "\n",
    "    verbose   = 0,\n",
    "    callbacks = [Monitoring(x_range=[0, 10000], refresh_steps=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the learned function\n",
    "plot_learned_landscape(model, data_points_train, data_labels_train, data_points_test, data_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
