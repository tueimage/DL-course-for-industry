\documentclass[notes]{beamer}          % print frame + notes
%\documentclass[notes=only]{beamer}     % only notes
%\documentclass{beamer}                 % only frames

\usecolortheme{beaver}

% Some commonly used packages
% (copied mainly from the Utrecht University theme: https://www.overleaf.com/project/5c900fa3bd9930036341116a)
\usepackage{ragged2e}  % `\justifying` text
\usepackage{booktabs}  % Tables
\usepackage{tabularx}
\usepackage{tikz}      % Diagrams
\usetikzlibrary{calc, shapes, backgrounds}
\usepackage{amsmath, amssymb, amsfonts, amsthm}
\usepackage{url}       % `\url`s
\usepackage{listings}  % Code listings
\usepackage{comment}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{bm}

% Mainly math commands
\newcommand{\vect}[1]{\bm{#1}}
\usepackage{amsfonts}% to get the \mathbb alphabet
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\C}{\field{C}}
\newcommand{\R}{\field{R}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}


% A variable used to exclude slides from the lecture version
\newif\iffull
\fullfalse
%\fulltrue

% Bibliography
\usepackage[uniquename=init,giveninits=true,maxcitenames=1,style=authortitle-comp]{biblatex}
\bibliography{lectures/references}

\setbeamertemplate{frametitle continuation}[from second][]

%Information to be included in the title page:
\title{Machine learning fundamentals}
\subtitle{Deep learning course for industry }
\author{Mitko Veta}
\institute{Eindhoven University of Technology


Department of Biomedical Engineering}
\date{2020}
  
 
\begin{document}
 
\frame{\titlepage}


\begin{frame}
\frametitle{Historical perspective}
\begin{center}
\includegraphics[height=7cm]{figures/deep_learning.png}
\end{center}
{\tiny Figure source: nvidia.com}
\end{frame}


\begin{frame}[allowframebreaks]{Course overview}

        \begin{itemize}
            \item 09.30 - 10.15 Machine learning fundamentals
            \item 10.30 - 11.15 From linear models to deep neural networks
            \item 11.30 - 12.00 Convolutional neural networks
            \item \textit{Lunch break}
	    \item 13.00 - 14.00 Training neural networks in your web browser
        \end{itemize}
    
\end{frame}

\begin{frame}
\frametitle{Learning goals}
\begin{itemize}
    \item Define machine learning.
    \item Introduce the conceptually simple yet practically useful linear model. 
    \item Discuss the central challenge of machine learning: generalisation.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{An example from my past work: nuclei area measurement}

\textbf{2010-2011}: An image processing pipeline of (mainly) mathematical morphology operators (e.g. the watershed algorithm).

\begin{figure}[ht]
        \begin{minipage}[b]{0.3\linewidth}
            \centering
            \includegraphics[width=\textwidth]{figures/pone_1.png}
        \end{minipage}
        \hspace{0.5cm}
        \begin{minipage}[b]{0.3\linewidth}
            \centering
            \includegraphics[width=\textwidth]{figures/pone_2.png}
        \end{minipage}
 \end{figure}

The design and validation of the processing pipeline took the better part of a year. \\~\

{\tiny Figure source: Veta et al. PLOS ONE 2012}

\end{frame}

\begin{frame}
\frametitle{An example from my past work: nuclei area measurement}

\textbf{2015}: A deep neural network for nuclei area measurement.

\begin{figure}[ht]
        \begin{minipage}[b]{0.3\linewidth}
            \centering
            \includegraphics[width=\textwidth]{figures/miccai_1.png}
        \end{minipage}
        \hspace{0.5cm}
        \begin{minipage}[b]{0.5\linewidth}
            \centering
            \includegraphics[width=\textwidth]{figures/miccai_2.png}
        \end{minipage}
 \end{figure}


The the training and validation of the deep neural network model took less than a week. \\~\

The results were more accurate than the the original method.  \\~\

{\tiny Figure source: Veta et al. MICCAI 2016}

\end{frame}

\iffull
\begin{frame}
\frametitle{An example from my past work: nuclei area measurement}

In the first case, I translated the domain knowledge of (medical) experts about nuclei appearance into a series of \textbf{manually written rules} that perform nuclei segmentation. \\~\

In the second case, I took a dataset of nuclei segmentations and fed it to a (deep) machine learning algorithm that \textbf{learned} how to directly measure nuclei size \textbf{from the provided examples}.

\end{frame}
\fi

\begin{frame}
\frametitle{The central premise of machine learning}

Learn ``computer programs'' from examples instead of manually writing rules. \\~\

\pause

Advantage: the same method (e.g. a neural network) can be used to solve a variety of different problems. \\~\

\begin{figure}[ht]
        \begin{minipage}[b]{0.45\linewidth}
            \centering
            \includegraphics[width=\textwidth]{figures/googlenet.png}
            \small{Siberian hustky vs. eskimo dog}
        \end{minipage}
        \hspace{0.5cm}
        \begin{minipage}[b]{0.35\linewidth}
            \centering
            \includegraphics[width=\textwidth]{figures/camelyon16.png}
	    \small{Normal vs. metastases}
        \end{minipage}
\end{figure}
\vfill
\tiny{Figures source: (left) Szegedy et al. arXiv 2014, (right) camelyon16.grand-challenge.org} 
\end{frame}

\iffull
\begin{frame}
\frametitle{The central premise of machine learning}
\begin{center}
\includegraphics[height=7cm]{figures/machine_learning.png}
\end{center}
{\tiny Figure source: xkcd.com}
\end{frame}
\fi

\begin{frame}[allowframebreaks]
\frametitle{What are the "examples"?}

Depends on the particular problem and task. \\~\

\textbf{Dataset:} cardiac MRI images. 

\textbf{Task:} detect if a specific pathology is present in each image. \\~\

\small{In this case, every image is an example and is associated with a binary target: 0 = ``healthy'', 1 = ``diseased'' (i.e. we want to classify each image as ``healthy'' or ``diseases''). } \\~\

\begin{center}
\includegraphics[width=7cm]{figures/examples_image.pdf}
\end{center}

\framebreak

\textbf{Dataset:} cardiac MRI images. 

\textbf{Task:} Segment the contours of the left ventricle \\~\

In this case, each pixel is an example and is associated with a binary target: 0 = ``background'', 1 = ``contour''.
\begin{center}
\includegraphics[width=7cm]{figures/examples_pixel.pdf}
\end{center}

\end{frame}


\begin{frame}
\frametitle{How are the ``examples'' represented?}
Traditionally with feature extraction:
\begin{center}
\includegraphics[width=11cm]{figures/representation_features.pdf}
\end{center}
\end{frame}

\begin{frame}
\frametitle{How are the ``examples'' represented?}
With raw pixel values (the \textit{de facto} standard for deep learning):
\begin{center}
\includegraphics[width=11cm]{figures/representation_raw.pdf}
\end{center}
\end{frame}

\begin{frame}
\frametitle{In summary...}

In order to design a machine learning algorithm for a specific task we are given a dataset of examples represented by $\mathbf{x}_i$. \\~\

Each example is (optionally) associated with a target $y_i$.  \\~\

The target can be categorical, such as class membership (e.g. $y_i = \{0, 1\}$), or continuous (e.g. area, volume etc.). \\~\

\end{frame}


\begin{frame}
\frametitle{Types of machine learning}

\begin{itemize}
	\item Unsupervised machine learning: given a dataset $x_i$, find ``some interesting properties''.
	\begin{itemize}
		\item Clustering: find groupings of $x_i$
		\item Density estimation: find $p(x_i)$
		\item Generative models. 
		\item $...$
	\end{itemize}
	\item Supervised machine learning: given a training dataset $\{x_i, y_i\}$, predict $\hat{y}_i$ of previously unseen samples.
	\begin{itemize}
		\item Regression:  the target variables $y_i$ are continuous. 
		\item Classification:  the target variables $y_i$ are continuous. 
		\item $...$
	\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{A simple machine learning model for regression}

The predictions $\hat{y}_i$ are a linear combination of the inputs: \\~\

\begin{center}
\includegraphics[width=0.75\textwidth]{figures/linear_regression.pdf} \\
$\hat{y} = \hat{w_0} + \sum_{j=1}^{p} x_j \hat{w_j}$
\end{center}

\end{frame}


\begin{frame}
\frametitle{Linear models are surprisingly useful and common}

Fetal weight estimate from ultrasound imaging: \\~\

fetal weight  =  $\hat{w}_0$ + $\hat{w}_1 \times$femur len. + $\hat{w}_2 \times$abdominal circ. + $\hat{w}_3 \times$head circ.

\begin{center}
\includegraphics[height=2.5cm]{figures/fetal_weight.png}
\end{center}
\vfill
{\tiny Figure source: my daughter }
\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{Linear model}
    \begin{itemize}
        \item Input vector $\vect{x}^T = (x_1, x_2, \ldots, x_p)$.
        \item Output $y$ predicted using the model \\
            \begin{center}
            $\hat{y} = \hat{w_0} + \sum_{j=1}^{p} x_j \hat{w_j}$
            \end{center}
        \iffull
        \item $\hat{w}_i$ $ (0 \leq i \leq p)$ are the parameters of the linear model.
	\framebreak
	\fi
        \item In vector form
            \begin{center}
            $\hat{y} = \hat{\vect{w}}^T\vect{x} = \vect{x}^T \hat{\vect{w}}$
            \end{center}
            \iffull
            using the fact that the scalar (inner) product of two vectors is a commutative operation.
            \fi
        \item  We assume that $w_0$ is in $\vect{w}$ and $1$ is included in $\vect{x}$.
        \iffull
        \item $\hat{y}$ is a scalar, but in general can be a $k$-vector $\hat{\vect{y}}$, in which case $\vect{w}$ becomes a $p \times k$ matrix of coefficients.
        \fi
    \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Linear model fit by least squares}
\frametitle{}
    \begin{itemize}
                \item We need to find coefficients $\hat{w_i}$ which minimise the error estimated with the {\bf residual sum of squares}
                \begin{center}
                    $$\mbox{RSS}(\vect{w}) = \sum_{i = 1}^{N}(y_i - \vect{x}_i^T \vect{w})^2$$
                \end{center}
                 assuming $N$ input-output pairs (the dataset).
        \item $\mbox{RSS}(\vect{w})$ is a quadratic function.
        \item A minimum always exists\iffull though not necessarily a unique one\fi.
    \end{itemize}

\end{frame}


\begin{frame}
\frametitle{Linear model fit by least squares}
    \begin{itemize}
        \item $\vect{y} = [y_1, y_2, \ldots, y_N]^T$ is the vector formed from the $N$ output values and $\vect{X}$ is an $N \times p$ matrix where each row corresponds to one example $\vect{x}_i$  \\
        % (\vect{y} - \vect{X}^T \vect{w})^2 =
        $$\mbox{RSS}(\vect{w}) =  (\vect{y} - \vect{X} \vect{w})^T (\vect{y} - \vect{X} \vect{w})$$
    \item    If $\vect{X}^T\vect{X}$ is non-singular there exists a unique solution given by
        $$\hat{\vect{w}} = (\vect{X}^T\vect{X})^{-1}\vect{X}^T \vect{y}$$
    \end{itemize}
\iffull
         {\bf Question}: Why not simply $\vect{y} -\vect{X}\vect{w}=  \vect{0}$ $\rightarrow$ $\vect{y} = \vect{X}\vect{w}$ $\rightarrow$ $\hat{\vect{w}} = \vect{X}^{-1} \vect{y}$?
\fi
\end{frame}


\begin{frame}{Linear model fit by least squares}
    \begin{itemize}
        \item For each input $\vect{x}_i$ there corresponds the fitted output $$\hat{y}_i = \hat{y}_i(\vect{x}_i) = \hat{\vect{w}}^T\vect{x}_i$$.
        \item This is called ``making a prediction'' for $\vect{x}_i$.

        \item The entire fitted surface (hyperplane) is fully characterised by the parameter vector $\hat{\vect{w}}$.

        \item After fitting the model, we can ``discard'' the training dataset. 
    \end{itemize}
\end{frame}

\begin{frame}
\frametitle{The ML workflow (thus far)}
\begin{itemize}
  \item Collect dataset $\{\mathbf{x}_i, y_i\}$.
  \item Assume a model for $\hat{y}$.
  \item Decide on an \textbf{error/loss function} that measures the ``goodness of fit'' of $\hat{y}$ to  $\{\mathbf{x}_i, y_i\}$.
  \item Fit the model to the data with an optimisation procedure (e.g. gradient-based optimisation).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{But what if a linear model is not enough?}
    \begin{center}
            \includegraphics[width=0.75\textwidth]{figures/linear_regression.pdf} \\
            $\hat{y} = \hat{w_0} + \sum_{i=1}^{p} x_i \hat{w_i}$ \\
            $\hat{y} = \vect{x}^T \hat{\vect{w}}$
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Polynomial regression}
    \begin{itemize}
        \item The linear regression algorithm can be generalised to include all polynomial functions instead of just the linear ones.
\iffull
        \item The linear regression model is then just a special case restricted to a polynomial of degree one: $\hat{y} = b + wx$.
\fi
        \item Moving to degree two we obtain: $\hat{y} = b + w_1 x + w_2 x^2$.
        \pause
        \begin{itemize}
            \item \textbf{This can be seen as adding a new feature $x^2$.}
            \item In fact, we can generalise this approach to create all sorts of hypothesis spaces, e.g.: $\hat{y} = b + w_1 x + w_2 \sin{(x)} + w_3 \sqrt{x}$.
        \end{itemize}
        \item The {\bf output} is still a {\bf linear} function of the parameters, so it can be fitted with least squares.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Polynomial regression}
    A comparison of a linear, degree-4, and degree-12 polynomials as predictors
    \begin{center}
        \includegraphics[width=0.3\textwidth]{figures/linear_regression_error.pdf}
        \includegraphics[width=0.3\textwidth]{figures/polynomial_regression_degree_4.pdf}
        \includegraphics[width=0.3\textwidth]{figures/polynomial_regression_degree_12.pdf}
    \end{center}
    \pause
    \begin{center}
        \includegraphics[width=0.3\textwidth]{figures/linear_regression_test_set.pdf}
        \includegraphics[width=0.3\textwidth]{figures/polynomial_regression_degree_4_test_set.pdf}
        \includegraphics[width=0.3\textwidth]{figures/polynomial_regression_degree_12_test_set.pdf}
    \end{center}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Generalisation}
    \begin{itemize}
        \item The central challenge in machine learning is to design an algorithm that will \textbf{perform well on new data} (different from the training set data).
        \item This ability is called {\bf generalisation}.
\iffull
        \item {\bf Training error} is the error computed on the training set.
\fi
\framebreak
        \item During the training (learning) we aim at reducing the training error.
        \item If that is the end goal, we only have an optimisation problem, not a machine learning one.
    \end{itemize}
    \begin{center}
            \includegraphics[width=0.45\textwidth]{figures/linear_regression_error.pdf}
            \includegraphics[width=0.45\textwidth]{figures/linear_regression_test_set.pdf}
    \end{center}
\end{frame}

\iffull
\begin{frame}
\frametitle{Generalisation error}
    \begin{itemize}
        \item {\bf Generalisation error}, also called {\bf test error} is defined as the expected error on new, previously unseen data.
        \item Unlike in simple optimisation, in machine learning our main goal is to minimise the {\bf generalisation error}.
        \item Usually the generalisation error is estimated by measuring the performance on a {\bf test data set} which must be \textbf{independent} from the training set.
    \end{itemize}
\end{frame}
\fi

\begin{frame}
\frametitle{Example: Linear regression}
 \begin{itemize}
        \item Previously, we trained the model by minimising the training error
        $$
        \frac{1}{m^{\mbox{(train)}}}\norm{\vect{X}^{\mbox{(train)}}\hat{\vect{w}} - \vect{y}^{\mbox{(train)}}}_2^2
        $$
        \item We would like actually to minimise the test error
         $$
        \frac{1}{m^{\mbox{(test)}}}\norm{\vect{X}^{\mbox{(test)}}\hat{\vect{w}} - \vect{y}^{\mbox{(test)}}}_2^2
        $$
\end{itemize}
\begin{center}
        \includegraphics[width=0.45\textwidth]{figures/linear_regression_error.pdf}
        \includegraphics[width=0.45\textwidth]{figures/linear_regression_test_set.pdf}
\end{center}
\end{frame}


\begin{frame}
\frametitle{Statistical learning theory}
    \begin{itemize}
        \item {\bf Statistical learning theory} provides methods to mathematically reason about the performance on the test set although we can observe only the training set.
        \item This is possible under some assumptions about the data sets
            \begin{itemize}
                \item The training and test data are generated by drawing from a probability distribution over data sets. We refer to that as {\bf data-generating process}.
                \item {\bf i.i.d. assumptions}
                    \begin{itemize}
                        \item Examples in each data sets are {\bf independent} from each other.
                        \item The training data set and the test data set are {\bf identically distributed}, i.e., drawn from the same probability distribution.
                    \end{itemize}
            \end{itemize}

    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Underfitting and overfitting}
    \begin{itemize}
        \item The factor that determines how well a machine algorithm will perform is its ability to
            \begin{enumerate}
                \item Make the training error small.
                \item Make the difference between the training and test error small.
            \end{enumerate}
        \item These two factors correspond to the two central challenges in machine learning: {\bf underfitting} and {\bf overfitting}.
        
	\iffull
        \item Underfitting occurs when the model is not able to produce a sufficiently small training error.
        \item Overfitting occurs when the gap between the training and test errors is too large.
	\fi
	
    \end{itemize}
\end{frame}

\iffull
\begin{frame}
\frametitle{Model capacity}
    \begin{itemize}
        \item A {\bf capacity of the model} is its ability to fit a wide variety of functions.

        \item Low capacity models struggle to fit the training set (underfitting).
        \item Models with high capacity have danger to overfit the training data (e.g., by ``memorising'' training samples).
	\pause
        \item The capacity can be controlled by choosing its {\bf hypothesis space}, i.e. the set of functions from which the learning algorithm is allowed to select the solution.
        \item Example: The linear regression algorithm has the set of all linear functions as its hypothesis space.

    \end{itemize}
\end{frame}
\fi

\begin{frame}
\frametitle{Underfitting and overfitting in polynomial estimation}
    \begin{itemize}
        \item Models with low capacity are not up to the task.
        \item Models with high-capacity can solve a complex task, but when the capacity is too high for the concrete (training) task there is the danger of overfitting.
    \end{itemize}
    \vfill
    \begin{center}
        \includegraphics[width=0.3\textwidth]{figures/linear_regression_test_set.pdf}
        \includegraphics[width=0.3\textwidth]{figures/polynomial_regression_degree_4_test_set.pdf}
        \includegraphics[width=0.3\textwidth]{figures/polynomial_regression_degree_12_test_set.pdf}
    \end{center}
\end{frame}

\begin{frame}
\frametitle{Generalisation and capacity}
    \begin{itemize}
        \item Simpler functions generalise more easily, but we still need to choose a sufficiently complex hypothesis (function) to obtain small training error.
        \item Typically training error decreases with the increase of the model capacity until an (asymptotic) value is reached.
        \item The generalisation error is U-shaped with the capacity range split in an underfitting and an overfitting zone. 
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Generalisation and capacity}
    \begin{center}
        \includegraphics[width=0.85\textwidth]{figures/generalization_gap.pdf}
    \end{center}
\end{frame}

\begin{comment}

\begin{frame}
\frametitle{Training set size}
    \begin{itemize}
        \item Training and generalisation error vary as the size of the training data set varies.
        \item Expected generalisation error never increases as the size of the training set increases.
        \iffull
        \item Any fixed parametric model will asymptotically approach an error value that exceeds the so called Bayes error.
        \fi
        \item It is possible for the model to have optimal capacity and still have a large gap between training and generalisation errors.
        \item In that case the gap usually can be reduced with increasing the number of training examples.
    \end{itemize}
\end{frame}


\begin{frame}
\frametitle{Training set size}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{figures/training_set_size.png}
    \end{center}
    \tiny{Figure source: deeplearningbook.org}
\end{frame}

\end{comment}

\begin{frame}
\frametitle{Regularisation}
    \begin{itemize}
        \item In addition to increasing and decreasing of the hypothesis space, i.e., the capacity, we can influence the learning algorithm by \textbf{giving preference to one solution over another in the hypothesis space}.
	\iffull
        \item In case both functions are eligible we can define a condition to express preference about one of the functions.
        \item The less preferred solution is chosen only if it gives significantly better performance with the training data.
        \pause
        \fi
	\item E.g., prefer smaller weights $\mathbf{w}$:
	\end{itemize}

	$$L(\mathbf{w}) = RSS(\mathbf{w}) + \lambda \mathbf{w}^{\intercal} \mathbf{w}$$
     
     
     \begin{center}
        \includegraphics[width=0.85\textwidth]{figures/regularization.png}
    \end{center}
\end{frame}


\begin{frame}
\frametitle{Summary}
\begin{itemize}
    \item Machine learning studies algorithms that learn from examples instead of relying on manually written rules. 
    \item The linear model is conceptually simple but practically useful and can be seen as the basic building block of neural networks.
    \item The central challenge in machine learning is to find a model that will perform well on new data. This ability is called generalisation.
\end{itemize}
\end{frame}

\end{document}